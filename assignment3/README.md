Details about this assignment can be found [on the course webpage](http://cs231n.github.io/), under Assignment #3 of Spring 2017.
This assignment consists of 5 exercises.

**Exercise on Image Captioning with Vanilla RNNS**:
* Introduced to the [Microsoft COCO dataset](http://cocodataset.org/#home) of images with labelled captions. This dataset will be used for the assessment of the image captioning algorithm using vanilla RNN
* Learn how the gradients are propagated through the RNN.
* Implement an image captioning RNN using numpy.
* Shows how the image to be captioned in fed into the RNN.
* Demonstrates how training and inference are implemented differently in the RNN.
* How to deal with varying caption length in the captioning task.

**Exercise on Image Captioning with LSTMs**:
* Learnt about the LSTM architecture and equations and how to implement it in numpy.
* Derive the gradients in LSTM.
* Introduced to BLEU for quantitatively evaluating the quality of the captioning.
* Train and tune an LSTM model to perform the image captioning task.

**Exercise on Network Visualization: Saliency Maps, Class Visualization, and Fooling Images**:

**Exercise on Style Transfer**:

**Exercise on Generative Adversarial Networks**

