Details about this assignment can be found [on the course webpage](http://cs231n.github.io/), under Assignment #3 of Spring 2017.
This assignment consists of 5 exercises.

**Exercise on Image Captioning with Vanilla RNNS**:
* Introduced to the [Microsoft COCO dataset](http://cocodataset.org/#home) of images with labelled captions. This dataset will be used for the assessment of the image captioning algorithm using vanilla RNN
* Learn how the gradients are propagated through the RNN.
* Implement an image captioning RNN using numpy.
* Shows how the image to be captioned in fed into the RNN.
* Demonstrates how training and inference are implemented differently in the RNN.
* How to deal with varying caption length in the captioning task.

**Exercise on Image Captioning with LSTMs**:
* Learnt about the LSTM architecture and equations and how to implement it in numpy.
* Derive the gradients in LSTM.
* Introduced to BLEU for quantitatively evaluating the quality of the captioning.
* Train and tune an LSTM model to perform the image captioning task.

**Exercise on Network Visualization: Saliency Maps, Class Visualization, and Fooling Images**:
* Introduced to Squeezenet and its architecture.
* Build a Saliency Map for each sample image to visualize which image pixel is influential in affecting the model's prediction.
* How to modify images that can fool a pretrained CNN to make wrong prediction of the images' class, when to the human eye, the images looked the same.
* Generate "images" of a class in the pretrained CNN. The "images" generated are generally not very clear, to the human eye, what the network is trying to depict.

**Exercise on Style Transfer**:

**Exercise on Generative Adversarial Networks**

